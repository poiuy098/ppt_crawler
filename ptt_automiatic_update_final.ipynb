{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ptt_automiatic_update_final.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"fZ9Vjp4xiQNs","colab_type":"code","colab":{},"outputId":"377276f8-0ea6-4393-93d0-d915a4b5547d"},"source":["import time\n","import json\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import requests\n","import re\n","from bs4 import BeautifulSoup \n","from selenium import webdriver\n","\n","\n","# # The following method catched all the PTT Food pages' URL from year 2004 to today\n","# urls = ['https://www.ptt.cc/bbs/Food/index{}.html'.format(str(i)) for i in range(7000,7003)]\n","# urls = urls[::-1]\n","# url_list = urls[1:2] # <-- Change the range to assign the pages to crawl\n","\n","res = requests.get('https://www.ptt.cc/bbs/Food/index.html', verify=False)\n","soup = BeautifulSoup(res.text,'html.parser')\n","\n","page_url_list = []\n","\n","#取得上一頁的ptt_page_url\n","page_url = soup.select('a.btn.wide')[1]\n","ptt_page_url = 'https://www.ptt.cc'+page_url.get('href')\n","res2 = requests.get(ptt_page_url)\n","new_url =BeautifulSoup(res2.text,'html.parser')\n","\n","#將上一頁的ptt_page_url的index分割\n","page_now = ptt_page_url.split('index')[1].replace(\".html\",\"\")\n","page_number = int(page_now)\n","page_number = page_number + 2\n","for i in range(page_number):\n","    web_url = f'https://www.ptt.cc/bbs/Food/index{i}.html'\n","    page_url_list.append(web_url)\n","\n","#取得最後2頁的ptt_page_url (可變)\n","url_list = page_url_list[-2:] \n","\n","start = time.time()\n","print('PTT program starts...')\n","# Get all the articles' URL in the given page\n","content_list = []\n","comments_list = []\n","except_list = []\n","\n","page = len(url_list)\n","for url in url_list:\n","    print('The', page, 'page', url,'starts:')\n","    #page -= 1\n","    res = requests.get(url, verify=False)\n","#     time.sleep(2)\n","    html_str = res.text\n","    soup = BeautifulSoup(html_str)\n","\n","    title_list = []\n","\n","    for i in soup.select('.title a'):\n","        route = 'https://www.ptt.cc' + i.get('href')\n","        title_list.append(route)\n","    # print(title_list)\n","    num = len(title_list)\n","    # Finally, we are able to get the contents\n","    for url2 in title_list:\n","        print('The page',page,num, url2)\n","        num -= 1\n","#         time.sleep(2)\n","        r2 = requests.get(url2)\n","        soup = BeautifulSoup(r2.text, 'html')\n","        main_content = soup.select('#main-content')\n","\n","        info = soup.select('.article-metaline')\n","        # The main content is the whole thing except the article's info\n","        info = [i.text for i in info]\n","        # Get the title\n","        if info != []:\n","            try:\n","                title = info[1]\n","                # Get the time\n","                date = info[2].replace('時間', '')\n","                info.insert(1, '看板Food')\n","                info_str = ''.join(info)\n","                # Clean the title, author, and other info\n","                main_content = main_content[0].text.replace(info_str, '')\n","\n","                # Split the whole thing into the main content and the comments sections\n","                main_content = main_content.split('※ 發信站: 批踢踢實業坊')\n","                content = main_content[0]  # This is the main content\n","                # Start clean the content\n","                # Clean out '\\n'\n","                content = content.replace('\\n', '')  \n","                #clean url\n","                url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?'\n","                text = re.sub(url_pattern, ' ', content)\n","                #clean all marks\n","                ptt_content = re.sub('[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]','',text)\n","                #clean XD\n","                content = re.sub('XDD*','',ptt_content) \n","            \n","                if len(main_content) >= 2:\n","                    \n","                    comments = main_content[1]  # This section is all the comments\n","                    comments = comments.split('.html')\n","                    # Get the IP address\n","                    ip_addr = comments[0].split('※ 文章網址')\n","                    ip_addr = ip_addr[0].split('來自: ')\n","                    ip_addr = ip_addr[1]\n","                    ip_addr = ip_addr.replace('\\n', '')\n","                    comments = comments[1]\n","                    comments = comments.split('\\n')\n","                    \n","                    # Count pushes and boos, and append all comments\n","                    push = 0\n","                    boo = 0\n","                    comment_count = 0\n","                    all_comments = ''\n","                    for i in comments:\n","                        if len(i) != 0:\n","                            if i[0] == '推':\n","                                push += 1\n","                            elif i[0] == '噓':\n","                                boo += 1\n","                            elif i[0] == '→':\n","                                comment_count += 1\n","                            i = i.split(' ')\n","                            if i[0] != '※':\n","                                i = i[2:-2]\n","                                i = ''.join(i)\n","                                all_comments += i\n","\n","                    total_comment_count = comment_count + push + boo\n","\n","                    content_list.append(\n","                        {'url': url2, 'title': title, 'time': date, 'ip': ip_addr, 'content': content, 'push': push,\n","                         'boo': boo, 'total': total_comment_count})\n","                    comments_list.append({'url': url2, 'comment': all_comments})\n","            except:\n","                except_list.append(url2)    \n","        else:\n","            except_list.append(url2)\n","    page -= 1\n","    \n","with open('ptt_new_10page_content.json', 'w', encoding='utf-8') as file:\n","    json.dump(content_list, file, ensure_ascii= False)\n","with open('ptt_new_10page_comment.json', 'w', encoding='utf-8') as file:\n","    json.dump(comments_list, file, ensure_ascii= False)\n","with open('ptt_new_10page_log.json', 'w', encoding='utf-8') as file:\n","    json.dump(except_list, file, ensure_ascii= False)\n"," \n","##############################################\n","\n","##讀取剛才爬好的檔案\n","file_name = './ptt_new_10page_content.json'\n","ptt_10 = pd.read_json(file_name , encoding = 'utf-8')\n","\n","#過濾掉標題有公告的列\n","ptt_new = ptt_10[ptt_10['title'].str.contains('公告') != True]    \n","    \n","#將ptt_id有(台灣的分割)\n","ip_cut = ptt_new['ip'].to_string()\n","ip_adress = ip_cut.split()\n","\n","#將取好乾淨ptt_ip存回df\n","ptt_new['ip'] = ip_adress[1:len(ip_adress):3]\n","\n","\n","##############################################\n","# Convert IP into city name\n","# Base on the city name convert into North(1), Middel(2), South(3), East(4), and others(0) \n","\n","ip_list = list(ptt_new['ip'])\n","\n","print('Convert IP into city name. Starts...')\n","\n","city_list = []\n","count = len(ip_list)\n","start = time.time()\n","#driver = webdriver.Chrome(ChromeDriverManager().install()) #For Mac OS users\n","driver = webdriver.Chrome(executable_path=\"chromedriver.exe\") #For Windows users\n","driver.get('https://www.ez2o.com/App/Net/IP')\n","for ip_addr in ip_list:\n","    print(count,'...')\n","    count -= 1\n","    elem = driver.find_element_by_xpath(\"//input[@id='QueryIP']\").clear()\n","    elem = driver.find_element_by_xpath(\"//input[@id='QueryIP']\")\n","    elem.send_keys(ip_addr) # ex: 218.173.71.162\n","    elem = driver.find_element_by_xpath(\"//button[@class='btn btn-primary']\")\n","    elem.click()\n","    elem = driver.find_element_by_xpath(\"//tbody/tr[@class='active'][3]/td[2]\")\n","    city_list.append(elem.text)\n","driver.close()\n","end = time.time()\n","\n","minute = round((end - start)/60)\n","second = round((end - start)%60)\n","\n","ptt_new['city'] = city_list\n","\n","print('Finished.')\n","\n","\n","##############################################\n","# Convert IP into city name\n","# Base on the city name convert into North(1), Middel(2), South(3), East(4), and others(0) \n","\n","print('Convert IP into city name. Starts...')\n","\n","north = ['Keelung','Keelung City','Zhubei','Taipei','New Taipei City',' Taipei County','Taoyuan District','Hsinchu','Yilan County','Yilan']\n","middle = ['Miaoli','Miaoli City','Yuanlin','Toufen Township','Taichung','Taichung City','Huwei','Nantou City','Puli Town','Douliu','Chang-hua','Yunlin County']\n","south = ['Chiayi City','Tainan City','Kaohsiung City','Pingtung City']\n","east = ['Hualien City','Hualien County','Taitung County','Taitung City']\n","\n","city_list = list(ptt_new['city'])\n","\n","area_code = []\n","for city in city_list:\n","    if city in north:\n","        area_code.append(1)\n","    elif city in middle:\n","        area_code.append(2)\n","    elif city in south:\n","        area_code.append(3)\n","    elif city in east:\n","        area_code.append(4)\n","    else:\n","        area_code.append(0)\n","\n","ptt_new['area'] = area_code\n","\n","# with open('ptt_new_10page_content_final.json'.format(file_name), 'w', encoding='utf-8') as file: # Change the output file name\n","#     ptt.to_json(file, force_ascii=False, orient='records')\n","print('Finished.')\n","\n","##############################################\n","##將ptt_time分割\n","#取week\n","weekday_cut = ptt_new['time'].to_string()\n","weekday = weekday_cut.split()\n","#將取好乾淨ptt_week另存df欄位\n","ptt_new['weekday'] = weekday[1:len(weekday):6]\n","\n","\n","#取month\n","month_cut = ptt_new['time'].to_string()\n","month = month_cut.split()\n","ptt_new['month'] = month[2:len(month):6]\n","\n","#取day\n","day_cut = ptt_new['time'].to_string()\n","day = day_cut.split()\n","ptt_new['day'] = day[3:len(weekday):6]\n","\n","#取time\n","time_cut = ptt_new['time'].to_string()\n","time = time_cut.split()\n","ptt_new['time2'] = time[4:len(weekday):6]\n"," \n","#取year\n","year_cut = ptt_new['time'].to_string()\n","year = year_cut.split()\n","ptt_new['year'] = weekday[5:len(weekday):6]\n","\n","##############################################\n","##轉換weekday成數字\n","ptt_new['weekday'].replace('Mon',1,inplace= True) #inplace = true改變原數據\n","ptt_new['weekday'].replace('Tue',2,inplace= True)\n","ptt_new['weekday'].replace('Wed',3,inplace= True)\n","ptt_new['weekday'].replace('Thu',4,inplace= True)\n","ptt_new['weekday'].replace('Fri',5,inplace= True)\n","ptt_new['weekday'].replace('Sat',6,inplace= True)\n","ptt_new['weekday'].replace('Sun',7,inplace= True)\n","\n","##轉換month成數字\n","ptt_new['month'].replace('Jan',1,inplace= True) #inplace = true改變原數據\n","ptt_new['month'].replace('Feb',2,inplace= True)\n","ptt_new['month'].replace('Mar',3,inplace= True)\n","ptt_new['month'].replace('Apr',4,inplace= True)\n","ptt_new['month'].replace('May',5,inplace= True)\n","ptt_new['month'].replace('Jun',6,inplace= True)\n","ptt_new['month'].replace('Jul',7,inplace= True)\n","ptt_new['month'].replace('Aug',8,inplace= True)\n","ptt_new['month'].replace('Sep',9,inplace= True)\n","ptt_new['month'].replace('Oct',10,inplace= True)\n","ptt_new['month'].replace('Nov',11,inplace= True)\n","ptt_new['month'].replace('Dec',12,inplace= True)\n","\n","##合併year,month,day欄位成date欄位\n","year = ptt_new['year'].astype('str')\n","month = ptt_new['month'].astype('str')\n","day = ptt_new['day'].astype('str')\n","ptt_new['date'] = year+'/'+month+'/'+day\n","\n","#drop time欄位\n","ptt_new = ptt_new.drop(\"time\", axis = 1)\n","\n","#修改time2名稱成time\n","ptt_new = ptt_new.rename(columns={'time2':'time'})\n","\n","##############################################\n","# Insert into the database\n","import mysql.connector\n","import time\n","cnx = mysql.connector.connect(user='june', password='june',\n","                              host='192.168.35.119',\n","                              database='ptt_db')\n","cursor = cnx.cursor()\n","query = (\"SELECT url FROM test_update;\")  # check the id list, and use it as the base to either UPDATE or INSERT \n","cursor.execute(query)\n","\n","url_list =[]\n","\n","for i in cursor:\n","    url_list.append(i[0])\n","    \n","\n","for i in range(len(ptt_new)):\n","    content_list = {'area': int(ptt_new.iloc[i]['area']),'city': str(ptt_new.iloc[i]['city']), \n","                        'content': str(ptt_new.iloc[i]['content']),'time': str(ptt_new.iloc[i]['time']),'title': str(ptt_new.iloc[i]['title']),\n","                        'day': str(ptt_new.iloc[i]['day']),'weekday': str(ptt_new.iloc[i]['weekday']),'year': str(ptt_new.iloc[i]['year']),\n","                        'month': str(ptt_new.iloc[i]['month']),'date': str(ptt_new.iloc[i]['date']),'time': str(ptt_new.iloc[i]['time']),\n","                        'push': int(ptt_new.iloc[i]['push']),'boo': int(ptt_new.iloc[i]['boo']),'total': int(ptt_new.iloc[i]['total']),'ip': str(ptt_new.iloc[i]['ip']),'url': str(ptt_new.iloc[i]['url'])}\n","    if ptt_new.iloc[i]['url'] in url_list: # Update  \n","        #Insert into Database\n","        update_article = \"UPDATE test_update SET date = %(date)s, year = %(year)s, month = %(month)s, day = %(day)s , area = %(area)s, city = %(city)s , weekday = %(weekday)s, time = %(time)s, title = %(title)s ,push = %(push)s, boo = %(boo)s, total = %(total)s, ip = %(ip)s, content = %(content)s WHERE url = %(url)s\"\n","        # Insert new article\n","        cursor.execute(update_article,content_list)\n","        # Make sure data is committed to the database\n","        cnx.commit()\n","        print(i,\":\",'Updated the database.')\n","    else: # Insert    \n","        #Insert into Database\n","        add_article = (\"INSERT test_update \"\n","                       \"(url,area,city,content,ip,date,time,title, weekday,year,month,day, push, boo, total) \"\n","                       \"VALUES (%(url)s, %(area)s, %(city)s, %(content)s, %(ip)s , %(date)s ,%(time)s, %(title)s ,%(weekday)s,%(year)s,%(month)s,%(day)s,%(push)s,%(boo)s ,%(total)s);\")\n","        # Insert new article\n","        cursor.execute(add_article,content_list)\n","        # Make sure data is committed to the database\n","        cnx.commit()\n","        print(i,\":\",'Inserted into the database.')\n","        \n","cursor.close()\n","cnx.close()\n","##############################################\n","        \n","end = time.time()\n","minute = round((end - start) / 60)\n","second = round((end - start) % 60)\n","print('Finished')\n","print('Total time:', minute, 'm', second, 's')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["PTT program starts...\n","The 2 page https://www.ptt.cc/bbs/Food/index7002.html starts:\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["The page 2 19 https://www.ptt.cc/bbs/Food/M.1560780060.A.5DF.html\n","The page 2 18 https://www.ptt.cc/bbs/Food/M.1560781133.A.536.html\n","The page 2 17 https://www.ptt.cc/bbs/Food/M.1560783695.A.213.html\n","The page 2 16 https://www.ptt.cc/bbs/Food/M.1560784602.A.95E.html\n","The page 2 15 https://www.ptt.cc/bbs/Food/M.1560786218.A.4B3.html\n","The page 2 14 https://www.ptt.cc/bbs/Food/M.1560791220.A.3C8.html\n","The page 2 13 https://www.ptt.cc/bbs/Food/M.1560825129.A.E03.html\n","The page 2 12 https://www.ptt.cc/bbs/Food/M.1560829325.A.FF2.html\n","The page 2 11 https://www.ptt.cc/bbs/Food/M.1560831312.A.482.html\n","The page 2 10 https://www.ptt.cc/bbs/Food/M.1560831887.A.36B.html\n","The page 2 9 https://www.ptt.cc/bbs/Food/M.1560832834.A.AE1.html\n","The page 2 8 https://www.ptt.cc/bbs/Food/M.1560834849.A.F16.html\n","The page 2 7 https://www.ptt.cc/bbs/Food/M.1560835661.A.828.html\n","The page 2 6 https://www.ptt.cc/bbs/Food/M.1560846524.A.E55.html\n","The page 2 5 https://www.ptt.cc/bbs/Food/M.1560847285.A.2DA.html\n","The page 2 4 https://www.ptt.cc/bbs/Food/M.1560847929.A.0B9.html\n","The page 2 3 https://www.ptt.cc/bbs/Food/M.1560848234.A.DAE.html\n","The page 2 2 https://www.ptt.cc/bbs/Food/M.1560850776.A.65A.html\n","The page 2 1 https://www.ptt.cc/bbs/Food/M.1560851213.A.DDA.html\n","The 1 page https://www.ptt.cc/bbs/Food/index7003.html starts:\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["The page 1 11 https://www.ptt.cc/bbs/Food/M.1560851252.A.239.html\n","The page 1 10 https://www.ptt.cc/bbs/Food/M.1560854186.A.E27.html\n","The page 1 9 https://www.ptt.cc/bbs/Food/M.1560854315.A.CF6.html\n","The page 1 8 https://www.ptt.cc/bbs/Food/M.1560854906.A.3BD.html\n","The page 1 7 https://www.ptt.cc/bbs/Food/M.1560856013.A.1DB.html\n","The page 1 6 https://www.ptt.cc/bbs/Food/M.1560856977.A.8E1.html\n","The page 1 5 https://www.ptt.cc/bbs/Food/M.1560861427.A.AC1.html\n","The page 1 4 https://www.ptt.cc/bbs/Food/M.1355673582.A.5F7.html\n","The page 1 3 https://www.ptt.cc/bbs/Food/M.1190944426.A.E6C.html\n","The page 1 2 https://www.ptt.cc/bbs/Food/M.1128132666.A.0FD.html\n","The page 1 1 https://www.ptt.cc/bbs/Food/M.1496532469.A.C36.html\n"],"name":"stdout"}]}]}